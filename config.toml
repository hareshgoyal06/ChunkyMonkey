# TLDR Configuration File
# Ollama + Pinecone configuration

[ollama]
# Ollama configuration for embeddings
base_url = "http://localhost:11434"
model = "nomic-embed-text:latest"  # Using the available embedding model
llm_model = "llama2:7b"  # LLM model for generating high-quality RAG answers

[pinecone]
api_key = "pcsk_4bSFWF_KpC7MSD3uKP9p13zGgd5HVgGgAB7ioqaCaeP3f4uT49CYNFcGnzgPD8aDRHW72E"
environment = "chunky-monkey-test"
index_name = "aped-4627-b74a"
# Custom host for your setup
host = "https://chunky-monkey-test-a2r60ad.svc.aped-4627-b74a.pinecone.io"

[search]
# Higher similarity threshold for better quality results
base_similarity_threshold = 0.5
fallback_threshold = 0.4
max_results_per_query = 10

# Enable advanced ML-based features
enable_semantic_search = true
enable_query_expansion = true
enable_content_filtering = true
enable_reranking = true

[chunking]
# Semantic chunking for better content understanding
max_chunk_size = 1500
min_chunk_size = 200
overlap_size = 200
use_semantic_chunking = true
respect_section_boundaries = true

[rag]
# Temporarily disable context quality assessment to test RAG functionality
enable_quality_assessment = false
enable_advanced_rag = true
enable_answer_validation = false
enable_semantic_expansion = true
enable_fallback_strategies = true
min_quality_threshold = 0.3
max_context_chunks = 15
enable_confidence_scoring = true
enable_source_attribution = true

[database]
path = "tldr.db" 